import torch
import torch.nn as nn
import torch.nn.functional as F

class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.v1=nn.Parameter(torch.ones(1*20))
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.v2=nn.Parameter(torch.ones(20*50))
        self.fc1 = nn.Linear(4*4*50, 500)
        self.v3=nn.Parameter(torch.ones(500))
        self.fc2 = nn.Linear(500, 10)
        self.v4=nn.Parameter(torch.ones(10))
        

    def forward(self, x):
        x = F.conv2d(x, torch.diag(self.v1).mm(self.conv1.weight.view(1*20,5*5)).view_as(self.conv1.weight), self.conv1.bias)
        x = F.relu(x)
        x = F.max_pool2d(x, 2, 2)
        x = F.conv2d(x, torch.diag(self.v2).mm(self.conv2.weight.view(20*50,5*5)).view_as(self.conv2.weight), self.conv2.bias)
        x = F.relu(x)
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4*4*50)
        x = F.linear(x, torch.transpose(self.v3*torch.transpose(self.fc1.weight,0,1),0,1), bias=self.fc1.bias)
        x = F.relu(x)
        x = F.linear(x, torch.diag(self.v4).mm(self.fc2.weight), bias=self.fc2.bias)
        return F.log_softmax(x, dim=1)

class CifarNet(nn.Module):
    def __init__(self):
        super(CifarNet, self).__init__()
        self.conv1=nn.Conv2d(3, 64, kernel_size=5)
        self.v1 = nn.Parameter(torch.ones(3*64))
        #self.bn_conv1 = nn.BatchNorm2d(64)
        self.conv2=nn.Conv2d(64, 64, kernel_size=5,padding=2)
        self.v2 = nn.Parameter(torch.ones(64*64))
        #self.bn_conv2 = nn.BatchNorm2d(64)
        self.fc1 = nn.Linear(2304, 384)
        self.v3 = nn.Parameter(torch.ones(384))
        self.drop1 = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(384, 192)
        self.v4 = nn.Parameter(torch.ones(192))
        self.drop2 = nn.Dropout(p=0.5)
        self.fc3 = nn.Linear(192, 10)
        self.v5 = nn.Parameter(torch.ones(10))
    
    def forward(self, x):
        x = F.conv2d(x, torch.diag(self.v1).mm(self.conv1.weight.view(3*64,5*5)).view_as(self.conv1.weight), self.conv1.bias)
        #x = self.bn_conv1(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 3, 2)
        x = F.conv2d(x, torch.diag(self.v2).mm(self.conv2.weight.view(64*64,5*5)).view_as(self.conv2.weight), self.conv2.bias, padding=2)
        #x = self.bn_conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 3, 2)
        x = x.view(-1, 2304)
        x = F.linear(x, torch.diag(self.v3).mm(self.fc1.weight), bias=self.fc1.bias)
        x = self.drop1(x)
        x = F.relu(x)
        x = F.linear(x, torch.diag(self.v4).mm(self.fc2.weight), bias=self.fc2.bias)
        x = self.drop2(x)
        x = F.relu(x)
        x = F.linear(x, torch.diag(self.v5).mm(self.fc3.weight), bias=self.fc3.bias)
        return F.log_softmax(x, dim=1)


class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)
        self.v1 = nn.Parameter(torch.ones(3*64))
        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)
        self.v2 = nn.Parameter(torch.ones(64*192))
        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)
        self.v3 = nn.Parameter(torch.ones(192*384))
        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)
        self.v4 = nn.Parameter(torch.ones(384*256))
        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)
        self.v5 = nn.Parameter(torch.ones(256*256))
        self.drop1 = nn.Dropout(p=0.5)
        self.fc1 = nn.Linear(256*6*6, 4096)
        self.v6 = nn.Parameter(torch.ones(4096))
        self.drop2 = nn.Dropout(p=0.5)
        self.fc2 = nn.Linear(4096, 4096)
        self.v7 = nn.Parameter(torch.ones(4096))
        self.fc3 = nn.Linear(4096, 1000)
        self.v8 = nn.Parameter(torch.ones(1000))

    def forward(self, x):
        x = F.conv2d(x,torch.transpose(self.v1*torch.transpose(self.conv1.weight.view(3*64,11*11),0,1),0,1).view_as(self.conv1.weight), self.conv1.bias, stride=4, padding=2)
        x = F.relu(x)
        x = F.max_pool2d(x, 3, 2)
        x = F.conv2d(x,torch.transpose(self.v2*torch.transpose(self.conv2.weight.view(64*192,5*5),0,1),0,1).view_as(self.conv2.weight), self.conv2.bias, padding=2)
        x = F.relu(x)
        x = F.max_pool2d(x, 3, 2)
        x = F.conv2d(x,torch.transpose(self.v3*torch.transpose(self.conv3.weight.view(192*384,3*3),0,1),0,1).view_as(self.conv3.weight), self.conv3.bias, padding=1)
        x = F.relu(x)
        x = F.conv2d(x,torch.transpose(self.v4*torch.transpose(self.conv4.weight.view(384*256,3*3),0,1),0,1).view_as(self.conv4.weight), self.conv4.bias, padding=1)
        x = F.relu(x)
        x = F.conv2d(x,torch.transpose(self.v5*torch.transpose(self.conv5.weight.view(256*256,3*3),0,1),0,1).view_as(self.conv5.weight), self.conv5.bias, padding=1)
        x = F.relu(x)
        x = F.max_pool2d(x, 3, 2)        
        x = F.adaptive_avg_pool2d(x, (6,6))
        x = x.view(-1, 256*6*6)
        x = self.drop1(x)
        x = F.linear(x, torch.transpose(self.v6*torch.transpose(self.fc1.weight,0,1),0,1), bias=self.fc1.bias)
        x = F.relu(x)
        x = self.drop2(x)
        x = F.linear(x, torch.transpose(self.v7*torch.transpose(self.fc2.weight,0,1),0,1), bias=self.fc2.bias)
        x = F.relu(x)
        x = F.linear(x, torch.transpose(self.v8*torch.transpose(self.fc3.weight,0,1),0,1), bias=self.fc3.bias)
        return F.log_softmax(x, dim=1)
